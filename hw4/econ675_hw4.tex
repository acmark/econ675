\documentclass[12pt]{article}
 \usepackage[margin=1in]{geometry}
 \usepackage{amsmath,amsthm,mathtools, amssymb,amsfonts,listings,color,graphicx,titlesec,lipsum,pdfpages,lscape}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\qsum}{\sum\limits_{i=1}^n}
\newcommand{\vpar}{\vspace{.3cm}}

\begin{document}
\title{Econ 675: HW 4}
\author{Erin Markiewitz}
\maketitle
\newpage
\tableofcontents
\newpage

\section{Estimating Equations}
This question considers identification under selection-on-observables for the generaic class of parameters
\begin{align*}
\theta_t(g) = \E[g(Y_i(t))], \ g \in G, \ t \in T
\end{align*}

where $G$ denotes a class of functions (e.g. $G = \{ \mathbf{1}(. \leq y) : y \in \R)\}$). Define the regression functions:
\begin{align*}
  p_t(\mathbf{X_i} = \P[T_i = t| \\
  \mathbf{X_i}]), e_t(g;\mathbf{X_i}) = \E[g(Y_i(t))|\mathbf{X_i}] = = \E[g(Y_i(t))|\mathbf{X_i}, T_i =1], \ g \in G, \ t \in T
\end{align*}

Assume Ignorability: $Y_i(t) \bot D_i(t) |\mathbf{X_i}$ and $0 < x < p_t(\X_i)$, for all $t \in T$ and for some fixed positive constant c.
\subsection{}
In the following section, we prove the validity of three moment cofitions for the generic class of parameters. The first moment condition is the Inverse Probabiltiy Weighting (IPW) moment condition.

\begin{align*}
\psi_{IPW,t}(\mathbf{Z}_i;\theta_t(g)) = \frac{D_i(t)\cdot g(Y_i(t))}{p_t(\X_i)} - \theta_t(g)
\end{align*}

To begin, take the expectation of the moment condition.

\begin{align*}
\E\left[\frac{D_i(t)\cdot g(Y_i(t))}{p_t(\X_i)}\right] - \theta_t(g)
\end{align*}

By the law of iterative expectations
\begin{align*}
  \E\left[\E\left[\frac{D_i(t) \cdot g(Y_i(t))}{p_t(\X_i)} |\X_i \right] \right] - \theta_t(g)
\end{align*}

\begin{align*}
  \E\left[\frac{1}{p_t(\X_i)}  \E\left[D_i(t) \cdot g(Y_i(t)) |\X_i \right] \right] - \theta_t(g)
\end{align*}

\begin{align*}
  \E\left[\frac{1}{p_t(\X_i)}  \E\left[D_i(t)|\X_i \right] \cdot \E\left[g(Y_i(t)) |\X_i \right] \right] - \theta_t(g)
\end{align*}


As $\E\left[D_i(t)|\X_i \right] = \Pr \left[D_i(t) = 1|\X_i \right] = \Pr \left[T_i= t|\X_i \right] =p_t(\X_i)$

\begin{align*}
  \E\left[\frac{p_t(\X_i)}{p_t(\X_i)} \cdot \E\left[g(Y_i(t)) |\X_i \right] \right] - \theta_t(g)
\end{align*}

which gives us our result,

\begin{align*}
  \E\left[\E\left[g(Y_i(t)) |\X_i \right]\right] - \theta_t(g) = \E\left[g(Y_i(t))\right] - \theta_t(g) = \theta_t(g) -\theta_t(g) = 0
\end{align*}


The second moment condition of this exercise is the Regression Imputation (1) moment condition:

\begin{align*}
\psi_{RI1,t}(\mathbf{Z}_i;\theta_t(g)) =  e_t(g;\X_i)- \theta_t(g)
\end{align*}

Take the expectation

\begin{align*}
\E\left[ e_t(g;\X_i)  \right]  - \theta_t(g)
\end{align*}

\begin{align*}
\E\left[\E\left[ g(Y_i(t)) | \X_i \right]  \right]  - \theta_t(g)
\end{align*}

\begin{align*}
\E\left[g(Y_i(t)) \right] - \theta_t(g) = \theta_t(g) -\theta_t(g) =0
\end{align*}

The second moment condition of this exercise is the Regression Imputation (2) moment condition, which includes inverse probabilty weighting:

\begin{align*}
\psi_{RI2,t}(\mathbf{Z}_i;\theta_t(g)) =  \frac{D_i(t) \cdot e_t(g;\X_i)}{p_t(\X_i)}- \theta_t(g)
\end{align*}

Take the expectation

\begin{align*}
\E \left[ \frac{D_i(t) \cdot e_t(g;\X_i)}{p_t(\X_i)}\right]  - \theta_t(g)
\end{align*}

iterate the expectation a bit

\begin{align*}
\E \left[ \frac{1}{p_t(\X_i)} p_t(\X_i) \cdot \E\left[ g(Y_i(t)) | \X_i \right] \right]  - \theta_t(g)
\end{align*}

which gives the result

\begin{align*}
\E\left[g(Y_i(t)) \right] - \theta_t(g) = \theta_t(g) -\theta_t(g) =0
\end{align*}

Last, we consider the doubly robust estimator's moment condition:

\begin{align*}
\psi_{DR,t}(\mathbf{Z}_i;\theta_t(g)) =  \frac{D_i(t) \cdot g(Y_i(t))}{p_t(\X_i)}- \theta_t(g) -  \frac{e_t(g;\X_i)}{p_t(\X_i)} \cdot (D_i(t) -  p_t(\X_i))
\end{align*}

Take expectations

\begin{align*}
 \E \left[\frac{D_i(t) \cdot g(Y_i(t))}{p_t(\X_i)} \right] - \theta_t(g) -  \E\left[\frac{e_t(g;\X_i)}{p_t(\X_i)} \cdot (D_i(t) -  p_t(\X_i))\right]
\end{align*}

From previous results the first two terms cancel,

\begin{align*}
- \E\left[\frac{D_i(t)\cdot e_t(g;\X_i) }{p_t(\X_i)} \right]  + \E \left[  e_t(g;\X_i) \right]
\end{align*}

The result follows from the law of iterated expectations.

\subsection{}
The IPW plug-in estimator:

\begin{align*}
\hat\psi_{IPW,t}(\mathbf{Z}_i;\theta_t(g)) = \frac{1}{n}\qsum\frac{D_i(t) \cdot g(Y_i) }{\hat p_t(\X_i)}
\end{align*}

Where $\hat p_t(\X_i)$ is the estimated propensity score from the first-stage regression of the treatment on the covariates.

To write down the RI1 plug-in estimator, start by putting a hat on it:

\begin{align*}
\hat\psi_{RI1,t}(\mathbf{Z}_i) =\frac{1}{n}\qsum \hat e_t(\X_i)
\end{align*}

where $\hat e_t(X_i) = \E[g(Y_i(t))|\X_i, T_i =t]$, the conditional expectation of the class of regression functions specified by $G$. We can rewrite the estimator above as:

\begin{align*}
\hat\psi_{RI1,t}(\mathbf{Z}_i) = \frac{1}{n}\qsum\frac{D_i(t) \cdot \hat e_t(\X_i)}{\hat p_t(\X_i)}
\end{align*}

To write down the RI1 plug-in estimator, just reweight using the estimated propensity score:

\begin{align*}
\hat\psi_{RI2,t}(\mathbf{Z}_i) = \frac{1}{n}\qsum\frac{D_i(t) \cdot \hat e_t(\X_i)}{\hat p_t(\X_i)}
\end{align*}

And the double robust plug in estimator

\begin{align*}
\hat\psi_{DR,t}(\mathbf{Z}_i) = \frac{1}{n}\qsum\frac{D_i(t) \cdot g(Y_i)}{\hat p_t(\X_i)}
-\frac{1}{n}\qsum\frac{\hat e_t(\X_i)}{\hat p_t(\X_i)} (D_i(t) - \hat p_t(\X_i))
\end{align*}

The relative performance of the estimators depends on the data generating process.
As the IPW and R2 plug in estimators use the estimated propensity score reweight the treatment effects, both estimators will be inconsistent in finite samples when the propensity score is very close to either one or zero. (If you are only estimating the treatement effect on the treated, it is sufficient that the propensity score is not degenerative with respect to 1.) The double robust estimators includes further safeguards against bias induced by misspecification but at the cost of imposing additional specification choices.

Then again, transparency is a key feature of an estimator - especially in policy analysis. Conditioning on covariates allow for specification of the propensity score without prior knowledge of the outcome variable/equation.

\subsection{}
(Just a hunch)
The estimating equations in section 1.1 can be used to estimate the variance of the potential outcome variables. First, we specify the function

\begin{align*}
  g(x)= (x - \E[x])^2, \ x \in \R
\end{align*}

and its finite sample analogue

\begin{align*}
  \hat g(x)= \frac{1}{n}\qsum(x_i - \frac{1}{n}\qsum x_i)^2 , \ x_i \in \X \in \R^n
\end{align*}

The validity of these moment conditions is established in section 1.1 more generally.
Under this specification of $g$, $\theta_t(g) = \V[Y_i(t)]$ and $e_t(g;\X_i) = \V[Y_i(t)|\X_i]$, the unconditional and conditional variance of the potential outcome of treatement t, respectively. This gives us the moment conditions:

\begin{align*}
\psi_{IPW,t}(\mathbf{Z}_i;\sigma_t^2) = \frac{D_i(t)\cdot   (Y_i(t) - \E[Y_i(t)])^2 }{p_t(\X_i)} - \sigma_t^2
\end{align*}

\begin{align*}
\psi_{RI1,t}(\mathbf{Z}_i;\sigma_t^2) =  \E[(Y_i(t) - \E[Y_i(t)])^2 | \X_i] - \sigma_t^2
\end{align*}

\begin{align*}
\psi_{RI2,t}(\mathbf{Z}_i;\sigma_t^2) =  \frac{D_i(t) \cdot \E[(Y_i(t) - \E[Y_i(t)])^2 | \X_i]}{p_t(\X_i)}- \sigma_t^2
\end{align*}

\begin{align*}
\psi_{DR,t}(\mathbf{Z}_i;\sigma_t^2) =  \frac{D_i(t) \cdot (Y_i(t) - \E[Y_i(t)])^2}{p_t(\X_i)}- \sigma_t^2-  \frac{\E[(Y_i(t) - \E[Y_i(t)])^2 | \X_i]}{p_t(\X_i)} \cdot (D_i(t) -  p_t(\X_i))
\end{align*}


Now in order to conduct the hypothesis test of $\mathbf{H}_0 : \sigma^2_t = \sigma^2$ we need to use the finite sample analogue of the $g$ function specified above. The variance of our moment conditions will be estimated using a simple GMM procedure. In this case, we use a two step procedure for the moment conditions that use IPW. In the first step, we estimate the propensity score and drop any observations with propensity scores sufficiently close to zero or one. For a given moment condition $M \in \{IPW,RI1,RI2,DR \} $  and treatment t we define the finite sample analogue as $\hat\psi_{M,t}$

So GMM is

\begin{align*}
  \hat{\mathbf{\Omega}}_{M,t}  = \frac{1}{n} \qsum \hat \psi_{M,t} \hat\psi_{M,t}'
\end{align*}

From Theorem 12.7.1 in Hansens's Econometrics text,

\begin{align*}
  \mathbf{\hat{V}}_{\psi,M,t} = (\hat \psi_{M,t} \hat{\mathbf{\Omega}}_{M,t}^{-1}   \hat\psi_{M,t}')^{-1}
\end{align*}

And so in order to conduct the hypothesis test we simply reject the null if $\hat\sigma_t^2$ is outside of the following confidence interval:

\begin{align*}
\mathbf{CI}_{\alpha} (\hat\sigma_t^2) = \left[\sigma^2  - \mathbf{\Phi}^{-1}(\frac{\alpha}{2}) \sqrt{\frac{\mathbf{\hat{V}}_{\psi,M,t}}{n}}   , \sigma^2  + \mathbf{\Phi}^{-1}(\frac{\alpha}{2}) \sqrt{\frac{\mathbf{\hat{V}}_{\psi,M,t}}{n}}  \right]
\end{align*}


\newpage
\section{}
In the following tables I present the ATE and ATT estimated using the Lalonde and PSID data. I am presenting only the Stata results. I have run the other results in R, although I was unable to get certain models to converge or even run in R. In discussion of the results, you see relative significant results with reasonable maginitude that appears to tell the story that there is significant returns to the NSW program. That said, there was significant problems getting the PSID IPW models to converge. This is an interesting situation that I do not have a good sense of what happened. Also matching across nearest neighbor and propensity score returns the same result for all of the specifications, which seems odd in general.

\begin{landscape}
\begin{table}
 \caption {Average Treatment Effects} \label{tab:title}
\begin{tabular}{lrrrrcrrrr}
\hline\hline
\multicolumn{1}{l}{\bfseries }&\multicolumn{4}{c}{\bfseries Experimental Data}&\multicolumn{1}{c}{\bfseries }&\multicolumn{4}{c}{\bfseries PSID Control}\tabularnewline
\cline{2-5} \cline{7-10}
\multicolumn{1}{l}{}&\multicolumn{1}{c}{$\hat{\tau}$}&\multicolumn{1}{c}{s.e.}&\multicolumn{1}{c}{C.I.}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{$\hat{\tau}$}&\multicolumn{1}{c}{s.e.}&\multicolumn{1}{c}{C.I.}&\multicolumn{1}{c}{}\tabularnewline
\hline
{\bfseries Mean Diff.}&&&&&&&&&\tabularnewline
 ~~&$1794.3424$&$670.99654$&$479.18915$&$3109.4956$&$  -15204.777$&$ 657.07631$&$-16492.647$&$-13916.908 $\tabularnewline
 \hline
 {\bfseries OLS}&&&&&&&&&\tabularnewline
 ~~a&$1582.1667$&$ 659.2457$&$290.04507$&$2874.2882$&$  6302.3954$&$ 1212.4566$&$ 3925.9805$&$ 8678.8104$  \tabularnewline
 ~~b&$1506.9012$&$657.31475$&$218.56428$&$2795.2381$&$  4699.259$&$ 1031.6669$&$ 2677.1918$&$ 6721.3262$  \tabularnewline
 ~~c&$1501.3732$&$662.43532$&$202.99999$&$2799.7464 $&$ 4284.342$&$ 1037.3931$&$ 2251.0516$&$ 6317.6324$  \tabularnewline
 \hline
 {\bfseries Reg. Impute}&&&&&&&&&\tabularnewline
 ~~a&$1462.2693$&$642.24087$&$ 203.4772$&$2721.0614$&$  -11195.037$&$ 1741.3261$&$-14608.036$&$-7782.0374$  \tabularnewline
 ~~b&$1454.1282$&$643.48562$&$192.89638$&$2715.36$&$ -10398.22$&$ 3293.3996$&$-16853.283$&$-3943.1565$  \tabularnewline
 ~~c&$1427.5263$&$642.95044$&$167.34339$&$2687.7091$&$-11920.18$&$3834.631$&$-19436.057$&$-4404.3033$  \tabularnewline
 \hline
 {\bfseries IPW}&&&&&&&&&\tabularnewline
 ~~a&$1537.3978$&$ 646.6316$&$269.99986$&$2804.7957$&$-13507.18$&$ 2800.1988$&$-18995.569$&$-8018.79$  \tabularnewline
 ~~b&$1469.6152$&$647.10472$&$201.28993$&$2737.9404$&$-6028.4906$&$3819.791$&$-13515.281$&$ 1458.2998$  \tabularnewline
~~c&$1468.1014$&$646.91873$&$200.14072$&$2736.0622$&$-6626.6908$&$ 0$&$-6626.6908$&$-6626.6908$  \tabularnewline
\hline
{\bfseries D. Robust}&&&&&&&&&\tabularnewline
~~a&$1537.3978$&$ 646.6316$&$269.99986$&$2804.7957$&$-13507.18$&$ 2800.1988$&$-18995.569$&$-8018.79$  \tabularnewline
~~b&$1469.6152$&$647.10472$&$201.28993$&$2737.9404$&$-6028.4906$&$3819.791$&$-13515.281$&$ 1458.2998$  \tabularnewline
~~c&$1468.1014$&$ 646.9188$&$ 200.1406$&$2736.0623$&$-6626.6908$&$ 0$&$-6626.6908$&$-6626.6908$  \tabularnewline
\hline
{\bfseries N1 Match}&&&&&&&&&\tabularnewline
~~a&$1829.7958$&$779.59802$&$301.78369$&$3357.8079$&$-15619.49$&$ 1153.3158$&$-17879.989$&$-13358.991$  \tabularnewline
~~b&$1875.8281$&$734.95291$&$435.32036$&$3316.3358$&$-9349.564$&$ 3974.7701$&$-17140.113$&$-1559.0146$  \tabularnewline
~~c&$1671.7386$&$726.06099$&$ 248.6591$&$3094.8182$&$-9561.9662$&$ 4033.5414$&$-17467.707$&$ -1656.225$  \tabularnewline
\hline
{\bfseries p Match}&&&&&&&&&\tabularnewline
~~a&$ 10421.27$&$4318.4516$&$1957.1047$&$18885.435$&$-10421.27$&$ 4318.4516$&$-18885.435$&$-1957.1047$  \tabularnewline
~~b&$ 10421.27$&$4318.4516$&$1957.1047$&$18885.435$&$-10421.27$&$ 4318.4516$&$-18885.435$&$-1957.1047$  \tabularnewline
~~c&$ 10421.27$&$4318.4516$&$1957.1047$&$18885.435$&$-10421.27$&$ 4318.4516$&$-18885.435$&$-1957.1047$  \tabularnewline
\hline
\end{tabular}
\end{table}
\end{landscape}
\newpage

\begin{landscape}
\begin{table}
 \caption {Average Treatment Effects on Treated} \label{tab:title}
\begin{tabular}{lrrrrcrrrr}
\hline\hline
\multicolumn{1}{l}{\bfseries }&\multicolumn{4}{c}{\bfseries Experimental Data}&\multicolumn{1}{c}{\bfseries }&\multicolumn{4}{c}{\bfseries PSID Control}\tabularnewline
\cline{2-5} \cline{7-10}
\multicolumn{1}{l}{}&\multicolumn{1}{c}{$\hat{\tau}$}&\multicolumn{1}{c}{s.e.}&\multicolumn{1}{c}{C.I.}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{$\hat{\tau}$}&\multicolumn{1}{c}{s.e.}&\multicolumn{1}{c}{C.I.}&\multicolumn{1}{c}{}\tabularnewline
\hline
{\bfseries Mean Diff.}&&&&&&&&&\tabularnewline
 ~~&$ 1794.3424$&$ 670.99654$&$ 479.18915$&$ 3109.4956   $&&  $-15204.777$&$ 657.07631$&$-16492.647$&$-13916.908$ \tabularnewline
 \hline
 {\bfseries OLS}&&&&&&&&&\tabularnewline
 ~~a&$ 1582.1667$&$659.2457$&$ 290.04507$&$ 2874.2882 $&&   $ 6302.3954$&$ 1212.4566$&$ 3925.9805$&$ 8678.8104$ \tabularnewline
 ~~b&$ 1506.9012$&$ 657.31475$&$ 218.56428$&$ 2795.2381  $&&   $4699.259$&$ 1031.6669$&$ 2677.1918$&$ 6721.3262  $ \tabularnewline
 ~~c&$ 1501.3732$&$ 662.43532$&$ 202.99999$&$ 2799.7464  $&&   $4284.342$&$ 1037.3931$&$ 2251.0516$&$ 6317.6324  $ \tabularnewline
 \hline
 {\bfseries Reg. Impute}&&&&&&&&&\tabularnewline
 ~~a&$ 1726.6021$&$ 688.76383$&$ 376.62496$&$ 3076.5792  $&&   $-12661.529$&$ 1852.7548$&$-16292.929$&$-9030.1299$ \tabularnewline
 ~~b&$ 1809.6967$&$ 693.86739$&$ 449.71661$&$ 3169.6768  $&&   $-11537.261$&$ 3539.2681$&$-18474.226$&$ -4600.295$ \tabularnewline
 ~~c&$ 1844.6059$&$694.8217$&$ 482.75536$&$ 3206.4564 $&&   $-13218.766$&$ 4119.5046$&$-21292.995$&$-5144.5369$ \tabularnewline
 \hline
 {\bfseries IPW}&&&&&&&&&\tabularnewline
 ~~a&$ 1765.8615$&$ 698.04009$&$ 397.70292$&$ 3134.0201  $&&   $-15249.872$&$ 3117.0044$&$-21359.201$&$-9140.5436$ \tabularnewline
 ~~b&$ 1741.4891$&$ 701.89105$&$ 365.78265$&$ 3117.1956  $&&   $-7712.6195$&$4223.102$&$-15989.899$&$ 564.66034  $ \tabularnewline
~~c&$ 1774.86$&$ 702.34115$&$ 398.27133$&$ 3151.4486  $&&   $-8139.2549$&$ 0$&$-8139.2549$&$-8139.2549        $ \tabularnewline
\hline
{\bfseries D. Robust}&&&&&&&&&\tabularnewline
~~a&$ 1765.8615$&$ 698.04009$&$ 397.70292$&$ 3134.0201   $&&   $-15249.872$&$ 3117.0044$&$-21359.201$&$-9140.5436$ \tabularnewline
~~b&$ 1741.4891$&$ 701.89105$&$ 365.78266$&$ 3117.1956   $&&   $-7712.6195$&$4223.102$&$-15989.899$&$ 564.66034  $ \tabularnewline
~~c&$ 1774.86$&$ 702.34104$&$ 398.27155$&$ 3151.4484  $&&   $-8139.2549$&$ 0$&$-8139.2549$&$-8139.2549        $ \tabularnewline
\hline
{\bfseries N1 Match}&&&&&&&&&\tabularnewline
~~a&$ 1558.1563$&$ 776.73016$&$ 35.765204$&$ 3080.5474   $&&   $ -16904.15$&$ 1217.6947$&$-19290.831$&$-14517.468$ \tabularnewline
~~b&$ 1731.6091$&$ 732.36262$&$ 296.17836$&$ 3167.0398   $&&   $-10196.024$&$ 4254.9459$&$-18535.718$&$-1856.3305$ \tabularnewline
~~c&$ 1137.4252$&$ 813.43663$&$-456.91054$&$2731.761  $&&   $ -10421.27$&$ 4318.4516$&$-18885.435$&$-1957.1047$ \tabularnewline
\hline
{\bfseries p Match}&&&&&&&&&\tabularnewline
~~a&$10421.27$&$ 4318.4516$&$ 1957.1047$&$ 18885.435  $&&   $ -10421.27$&$ 4318.4516$&$-18885.435$&$-1957.1047$ \tabularnewline
~~b&$10421.27$&$ 4318.4516$&$ 1957.1047$&$ 18885.435  $&&   $ -10421.27$&$ 4318.4516$&$-18885.435$&$-1957.1047$ \tabularnewline
~~c&$10421.27$&$ 4318.4516$&$ 1957.1047$&$ 18885.435  $&&   $ -10421.27$&$ 4318.4516$&$-18885.435$&$-1957.1047$ \tabularnewline
\hline
\end{tabular}
\end{table}
\end{landscape}


\newpage
\section{}
\subsection{}
While I only include the graphs from the my stata run, I was able to run the exercise in R and provide the table below. The results are quite similar in terms of magnitude. In particular, I found that excluding $z_i$ from the regression led to a biased estimate of the coefficient on $x_i$ which affects the estimate coefficient in the model selection process.


\includegraphics[totalheight=5cm]{hw4_q3_bhat_stata.png}
\includegraphics[totalheight=5cm]{hw4_q3_btilde_stata.png}\\
\includegraphics[totalheight=5cm]{hw4_q3_bcheck_stata.png}
\begin{table}[ht]
  \caption {Stata Output:} \label{tab:title}
\centering
\begin{tabular}{lcccc}
  \hline
&        mean&          sd&         min&         max\\
\hline
$\hat\beta$    &    .5153&    .2900&   -.3544&    1.436\\
$\check\beta$   &    .5415&    .3413&   -.3544&    1.705\\
$\tilde\beta$    &    1.358&    .1663&    .8311&    1.950\\
\hline
\end{tabular}
\end{table}

\par
\begin{table}[ht]
  \caption {R Output:} \label{tab:title}
\centering
\begin{tabular}{lccc}
  \hline
 & Mean & Min. & Max. \\
  \hline
$\hat\beta$   & 0.49 & 0.49 &  1.43 \\
  $\tilde\beta$   & 1.42 & 1.43 & 1.86 \\
 $\check\beta$  & 0.49 & 0.51 & 1.53 \\
   \hline
\end{tabular}
\end{table}

\subsection{}

The model selection procedure drastically reduces the coverage rate of the estimator As the process shifts the location of the point estimate, it significantly reduces the coverage rate of the confidence interval. I think my coverage rate for beta hat is incorrectly estimated, it is far too low. \\


Stata output:
\begin{tabular}{lc}
  \hline
&        Coverage Rate \\
\hline
$\hat\beta$    &    0.201 \\
$\check\beta$   &     0.244 \\
$\tilde\beta$    &   0 \\
\hline
\end{tabular}\\


R output:
\begin{tabular}{rr}
  \hline
 & Coverage Rate \\
  \hline
$\hat\beta$  & 0.74 \\
  $\tilde\beta$  & 0.00 \\
  $\check\beta$  & 0.00 \\
   \hline
\end{tabular}



\newpage
\section{Code Appendix}
\tiny
\subsection*{Stata}
\lstinputlisting{econ675_hw4.do}
\subsection*{R}
\subsubsection*{Question 2}
\lstinputlisting{hw4_q2.R}
\subsubsection*{Question 3}
\lstinputlisting{hw4_q3.R}
\end{document}
